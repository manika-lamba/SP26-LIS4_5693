<!DOCTYPE html>
<html lang="en"><head>
<script src="index_files/libs/clipboard/clipboard.min.js"></script>
<script src="index_files/libs/quarto-html/tabby.min.js"></script>
<script src="index_files/libs/quarto-html/popper.min.js"></script>
<script src="index_files/libs/quarto-html/tippy.umd.min.js"></script>
<link href="index_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="index_files/libs/quarto-html/light-border.css" rel="stylesheet">
<link href="index_files/libs/quarto-html/quarto-html.min.css" rel="stylesheet" data-mode="light">
<link href="index_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.5.57">

  <meta name="author" content="Dr.&nbsp;Manika Lamba">
  <title>Text Pre-processing</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="index_files/libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="index_files/libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="index_files/libs/revealjs/dist/theme/quarto.css">
  <link rel="stylesheet" href="styles.css">
  <link href="index_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="index_files/libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="index_files/libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="index_files/libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/all.css" rel="stylesheet">
  <link href="index_files/libs/revealjs/plugin/reveal-chalkboard/style.css" rel="stylesheet">
  <link href="index_files/libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">

  .callout {
    margin-top: 1em;
    margin-bottom: 1em;  
    border-radius: .25rem;
  }

  .callout.callout-style-simple { 
    padding: 0em 0.5em;
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
    display: flex;
  }

  .callout.callout-style-default {
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
  }

  .callout .callout-body-container {
    flex-grow: 1;
  }

  .callout.callout-style-simple .callout-body {
    font-size: 1rem;
    font-weight: 400;
  }

  .callout.callout-style-default .callout-body {
    font-size: 0.9rem;
    font-weight: 400;
  }

  .callout.callout-titled.callout-style-simple .callout-body {
    margin-top: 0.2em;
  }

  .callout:not(.callout-titled) .callout-body {
      display: flex;
  }

  .callout:not(.no-icon).callout-titled.callout-style-simple .callout-content {
    padding-left: 1.6em;
  }

  .callout.callout-titled .callout-header {
    padding-top: 0.2em;
    margin-bottom: -0.2em;
  }

  .callout.callout-titled .callout-title  p {
    margin-top: 0.5em;
    margin-bottom: 0.5em;
  }
    
  .callout.callout-titled.callout-style-simple .callout-content  p {
    margin-top: 0;
  }

  .callout.callout-titled.callout-style-default .callout-content  p {
    margin-top: 0.7em;
  }

  .callout.callout-style-simple div.callout-title {
    border-bottom: none;
    font-size: .9rem;
    font-weight: 600;
    opacity: 75%;
  }

  .callout.callout-style-default  div.callout-title {
    border-bottom: none;
    font-weight: 600;
    opacity: 85%;
    font-size: 0.9rem;
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-default div.callout-content {
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-simple .callout-icon::before {
    height: 1rem;
    width: 1rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 1rem 1rem;
  }

  .callout.callout-style-default .callout-icon::before {
    height: 0.9rem;
    width: 0.9rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 0.9rem 0.9rem;
  }

  .callout-title {
    display: flex
  }
    
  .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  .callout.no-icon::before {
    display: none !important;
  }

  .callout.callout-titled .callout-body > .callout-content > :last-child {
    padding-bottom: 0.5rem;
    margin-bottom: 0;
  }

  .callout.callout-titled .callout-icon::before {
    margin-top: .5rem;
    padding-right: .5rem;
  }

  .callout:not(.callout-titled) .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  /* Callout Types */

  div.callout-note {
    border-left-color: #4582ec !important;
  }

  div.callout-note .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEU0lEQVRYCcVXTWhcVRQ+586kSUMMxkyaElstCto2SIhitS5Ek8xUKV2poatCcVHtUlFQk8mbaaziwpWgglJwVaquitBOfhQXFlqlzSJpFSpIYyXNjBNiTCck7x2/8/LeNDOZxDuEkgOXe++553zfefee+/OYLOXFk3+1LLrRdiO81yNqZ6K9cG0P3MeFaMIQjXssE8Z1JzLO9ls20MBZX7oG8w9GxB0goaPrW5aNMp1yOZIa7Wv6o2ykpLtmAPs/vrG14Z+6d4jpbSKuhdcSyq9wGMPXjonwmESXrriLzFGOdDBLB8Y6MNYBu0dRokSygMA/mrun8MGFN3behm6VVAwg4WR3i6FvYK1T7MHo9BK7ydH+1uurECoouk5MPRyVSBrBHMYwVobG2aOXM07sWrn5qgB60rc6mcwIDJtQrnrEr44kmy+UO9r0u9O5/YbkS9juQckLed3DyW2XV/qWBBB3ptvI8EUY3I9p/67OW+g967TNr3Sotn3IuVlfMLVnsBwH4fsnebJvyGm5GeIUA3jljERmrv49SizPYuq+z7c2H/jlGC+Ghhupn/hcapqmcudB9jwJ/3jvnvu6vu5lVzF1fXyZuZZ7U8nRmVzytvT+H3kilYvH09mLWrQdwFSsFEsxFVs5fK7A0g8gMZjbif4ACpKbjv7gNGaD8bUrlk8x+KRflttr22JEMRUbTUwwDQScyzPgedQHZT0xnx7ujw2jfVfExwYHwOsDTjLdJ2ebmeQIlJ7neo41s/DrsL3kl+W2lWvAga0tR3zueGr6GL78M3ifH0rGXrBC2aAR8uYcIA5gwV8zIE8onoh8u0Fca/ciF7j1uOzEnqcIm59sEXoGc0+z6+H45V1CvAvHcD7THztu669cnp+L0okAeIc6zjbM/24LgGM1gZk7jnRu1aQWoU9sfUOuhrmtaPIO3YY1KLLWZaEO5TKUbMY5zx8W9UJ6elpLwKXbsaZ4EFl7B4bMtDv0iRipKoDQT2sNQI9b1utXFdYisi+wzZ/ri/1m7QfDgEuvgUUEIJPq3DhX/5DWNqIXDOweC2wvIR90Oq3lDpdMIgD2r0dXvGdsEW5H6x6HLRJYU7C69VefO1x8Gde1ZFSJLfWS1jbCnhtOPxmpfv2LXOA2Xk2tvnwKKPFuZ/oRmwBwqRQDcKNeVQkYcOjtWVBuM/JuYw5b6isojIkYxyYAFn5K7ZBF10fea52y8QltAg6jnMqNHFBmGkQ1j+U43HMi2xMar1Nv0zGsf1s8nUsmUtPOOrbFIR8bHFDMB5zL13Gmr/kGlCkUzedTzzmzsaJXhYawnA3UmARpiYj5ooJZiUoxFRtK3X6pgNPv+IZVPcnwbOl6f+aBaO1CNvPW9n9LmCp01nuSaTRF2YxHqZ8DYQT6WsXT+RD6eUztwYLZ8rM+rcPxamv1VQzFUkzFXvkiVrySGQgJNvXHJAxiU3/NwiC03rSf05VBaPtu/Z7/B8Yn/w7eguloAAAAAElFTkSuQmCC');
  }

  div.callout-note.callout-style-default .callout-title {
    background-color: #dae6fb
  }

  div.callout-important {
    border-left-color: #d9534f !important;
  }

  div.callout-important .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEKklEQVRYCcVXTWhcVRS+575MJym48A+hSRFr00ySRQhURRfd2HYjk2SSTokuBCkU2o0LoSKKraKIBTcuFCoidGFD08nkBzdREbpQ1EDNIv8qSGMFUboImMSZd4/f9zJv8ibJMC8xJQfO3HPPPef7zrvvvnvviIkpC9nsw0UttFunbUhpFzFtarSd6WJkStVMw5xyVqYTvkwfzuf/5FgtkVoB0729j1rjXwThS7Vio+Mo6DNnvLfahoZ+i/o32lULuJ3NNiz7q6+pyAUkJaFF6JwaM2lUJlV0MlnQn5aTRbEu0SEqHUa0A4AdiGuB1kFXRfVyg5d87+Dg4DL6m2TLAub60ilj7A1Ec4odSAc8X95sHh7+ZRPCFo6Fnp7HfU/fBng/hi10CjCnWnJjsxvDNxWw0NfV6Rv5GgP3I3jGWXumdTD/3cbEOP2ZbOZp69yniG3FQ9z1jD7bnBu9Fc2tKGC2q+uAJOQHBDRiZX1x36o7fWBs7J9ownbtO+n0/qWkvW7UPIfc37WgT6ZGR++EOJyeQDSb9UB+DZ1G6DdLDzyS+b/kBCYGsYgJbSQHuThGKRcw5xdeQf8YdNHsc6ePXrlSYMBuSIAFTGAtQo+VuALo4BX83N190NWZWbynBjhOHsmNfFWLeL6v+ynsA58zDvvAC8j5PkbOcXCMg2PZFk3q8MjI7WAG/Dp9AwP7jdGBOOQkAvlFUB+irtm16I1Zw9YBcpGTGXYmk3kQIC/Cds55l+iMI3jqhjAuaoe+am2Jw5GT3Nbz3CkE12NavmzN5+erJW7046n/CH1RO/RVa8lBLozXk9uqykkGAyRXLWlLv5jyp4RFsG5vGVzpDLnIjTWgnRy2Rr+tDKvRc7Y8AyZq10jj8DqXdnIRNtFZb+t/ZRtXcDiVnzpqx8mPcDWxgARUqx0W1QB9MeUZiNrV4qP+Ehc+BpNgATsTX8ozYKL2NtFYAHc84fG7ndxUPr+AR/iQSns7uSUufAymwDOb2+NjK27lEFocm/EE2WpyIy/Hi66MWuMKJn8RvxIcj87IM5Vh9663ziW36kR0HNenXuxmfaD8JC7tfKbrhFr7LiZCrMjrzTeGx+PmkosrkNzW94ObzwocJ7A1HokLolY+AvkTiD/q1H0cN48c5EL8Crkttsa/AXQVDmutfyku0E7jShx49XqV3MFK8IryDhYVbj7Sj2P2eBxwcXoe8T8idsKKPRcnZw1b+slFTubwUwhktrfnAt7J++jwQtLZcm3sr9LQrjRzz6cfMv9aLvgmnAGvpoaGLxM4mAEaLV7iAzQ3oU0IvD5x9ix3yF2RAAuYAOO2f7PEFWCXZ4C9Pb2UsgDeVnFSpbFK7/IWu7TPTvBqzbGdCHOJQSxiEjt6IyZmxQyEJHv6xyQsYk//moVFsN2zP6fRImjfq7/n/wFDguUQFNEwugAAAABJRU5ErkJggg==');
  }

  div.callout-important.callout-style-default .callout-title {
    background-color: #f7dddc
  }

  div.callout-warning {
    border-left-color: #f0ad4e !important;
  }

  div.callout-warning .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAETklEQVRYCeVWW2gcVRg+58yaTUnizqbipZeX4uWhBEniBaoUX1Ioze52t7sRq6APio9V9MEaoWlVsFasRq0gltaAPuxms8lu0gcviE/FFOstVbSIxgcv6SU7EZqmdc7v9+9mJtNks51NTUH84ed889/PP+cmxP+d5FIbMJmNbpREu4WUkiTtCicKny0l1pIKmBzovF2S+hIJHX8iEu3hZJ5lNZGqyRrGSIQpq15AzF28jgpeY6yk6GVdrfFqdrD6Iw+QlB8g0YS2g7dyQmXM/IDhBhT0UCiRf59lfqmmDvzRt6kByV/m4JjtzuaujMUM2c5Z2d6JdKrRb3K2q6mA+oYVz8JnDdKPmmNthzkAk/lN63sYPgevrguc72aZX/L9C6x09GYyxBgCX4NlvyGUHOKELlm5rXeR1kchuChJt4SSwyddZRXgvwMGvYo4QSlk3/zkHD8UHxwVJA6zjZZqP8v8kK8OWLnIZtLyCAJagYC4rTGW/9Pqj92N/c+LUaAj27movwbi19tk/whRCIE7Q9vyI6yvRpftAKVTdUjOW40X3h5OXsKCdmFcx0xlLJoSuQngnrJe7Kcjm4OMq9FlC7CMmScQANuNvjfP3PjGXDBaUQmbp296S5L4DrpbrHN1T87ZVEZVCzg1FF0Ft+dKrlLukI+/c9ENo+TvlTDbYFvuKPtQ9+l052rXrgKoWkDAFnvh0wTOmYn8R5f4k/jN/fZiCM1tQx9jQQ4ANhqG4hiL0qIFTGViG9DKB7GYzgubnpofgYRwO+DFjh0Zin2m4b/97EDkXkc+f6xYAPX0KK2I/7fUQuwzuwo/L3AkcjugPNixC8cHf0FyPjWlItmLxWw4Ou9YsQCr5fijMGoD/zpdRy95HRysyXA74MWOnscpO4j2y3HAVisw85hX5+AFBRSHt4ShfLFkIMXTqyKFc46xdzQM6XbAi702a7sy04J0+feReMFKp5q9esYLCqAZYw/k14E/xcLLsFElaornTuJB0svMuJINy8xkIYuL+xPAlWRceH6+HX7THJ0djLUom46zREu7tTkxwmf/FdOZ/sh6Q8qvEAiHpm4PJ4a/doJe0gH1t+aHRgCzOvBvJedEK5OFE5jpm4AGP2a8Dxe3gGJ/pAutug9Gp6he92CsSsWBaEcxGx0FHytmIpuqGkOpldqNYQK8cSoXvd+xLxXADw0kf6UkJNFtdo5MOgaLjiQOQHcn+A6h5NuL2s0qsC2LOM75PcF3yr5STuBSAcGG+meA14K/CI21HcS4LBT6tv0QAh8Dr5l93AhZzG5ZJ4VxAqdZUEl9z7WJ4aN+svMvwHHL21UKTd1mqvChH7/Za5xzXBBKrUcB0TQ+Ulgkfbi/H/YT5EptrGzsEK7tR1B7ln9BBwckYfMiuSqklSznIuoIIOM42MQO+QnduCoFCI0bpkzjCjddHPN/F+2Yu+sd9bKNpVwHhbS3LluK/0zgfwD0xYI5dXuzlQAAAABJRU5ErkJggg==');
  }

  div.callout-warning.callout-style-default .callout-title {
    background-color: #fcefdc
  }

  div.callout-tip {
    border-left-color: #02b875 !important;
  }

  div.callout-tip .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAADr0lEQVRYCe1XTWgTQRj9ZjZV8a9SPIkKgj8I1bMHsUWrqYLVg4Ue6v9BwZOxSYsIerFao7UiUryIqJcqgtpimhbBXoSCVxUFe9CTiogUrUp2Pt+3aUI2u5vdNh4dmMzOzHvvezuz8xNFM0mjnbXaNu1MvFWRXkXEyE6aYOYJpdW4IXuA4r0fo8qqSMDBU0v1HJUgVieAXxzCsdE/YJTdFcVIZQNMyhruOMJKXYFoLfIfIvVIMWdsrd+Rpd86ZmyzzjJmLStqRn0v8lzkb4rVIXvnpScOJuAn2ACC65FkPzEdEy4TPWRLJ2h7z4cArXzzaOdKlbOvKKX25Wl00jSnrwVxAg3o4dRxhO13RBSdNvH0xSARv3adTXbBdTf64IWO2vH0LT+cv4GR1DJt+DUItaQogeBX/chhbTBxEiZ6gftlDNXTrvT7co4ub5A6gp9HIcHvzTa46OS5fBeP87Qm0fQkr4FsYgVQ7Qg+ZayaDg9jhg1GkWj8RG6lkeSacrrHgDaxdoBiZPg+NXV/KifMuB6//JmYH4CntVEHy/keA6x4h4CU5oFy8GzrBS18cLJMXcljAKB6INjWsRcuZBWVaS3GDrqB7rdapVIeA+isQ57Eev9eCqzqOa81CY05VLd6SamW2wA2H3SiTbnbSxmzfp7WtKZkqy4mdyAlGx7ennghYf8voqp9cLSgKdqNfa6RdRsAAkPwRuJZNbpByn+RrJi1RXTwdi8RQF6ymDwGMAtZ6TVE+4uoKh+MYkcLsT0Hk8eAienbiGdjJHZTpmNjlbFJNKDVAp2fJlYju6IreQxQ08UJDNYdoLSl6AadO+fFuCQqVMB1NJwPm69T04Wv5WhfcWyfXQB+wXRs1pt+nCknRa0LVzSA/2B+a9+zQJadb7IyyV24YAxKp2Jqs3emZTuNnKxsah+uabKbMk7CbTgJx/zIgQYErIeTKRQ9yD9wxVof5YolPHqaWo7TD6tJlh7jQnK5z2n3+fGdggIOx2kaa2YI9QWarc5Ce1ipNWMKeSG4DysFF52KBmTNMmn5HqCFkwy34rDg05gDwgH3bBi+sgFhN/e8QvRn8kbamCOhgrZ9GJhFDgfcMHzFb6BAtjKpFhzTjwv1KCVuxHvCbsSiEz4CANnj84cwHdFXAbAOJ4LTSAawGWFn5tDhLMYz6nWeU2wJfIhmIJBefcd/A5FWQWGgrWzyORZ3Q6HuV+Jf0Bj+BTX69fm1zWgK7By1YTXchFDORywnfQ7GpzOo6S+qECrsx2ifVQAAAABJRU5ErkJggg==');
  }

  div.callout-tip.callout-style-default .callout-title {
    background-color: #ccf1e3
  }

  div.callout-caution {
    border-left-color: #fd7e14 !important;
  }

  div.callout-caution .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAACV0lEQVRYCdVWzWoUQRCuqp2ICBLJXgITZL1EfQDBW/bkzUMUD7klD+ATSHBEfAIfQO+iXsWDxJsHL96EHAwhgzlkg8nBg25XWb0zIb0zs9muYYWkoKeru+vn664fBqElyZNuyh167NXJ8Ut8McjbmEraKHkd7uAnAFku+VWdb3reSmRV8PKSLfZ0Gjn3a6Xlcq9YGb6tADjn+lUfTXtVmaZ1KwBIvFI11rRXlWlatwIAAv2asaa9mlB9wwygiDX26qaw1yYPzFXg2N1GgG0FMF8Oj+VIx7E/03lHx8UhvYyNZLN7BwSPgekXXLribw7w5/c8EF+DBK5idvDVYtEEwMeYefjjLAdEyQ3M9nfOkgnPTEkYU+sxMq0BxNR6jExrAI31H1rzvLEfRIdgcv1XEdj6QTQAS2wtstEALLG1yEZ3QhH6oDX7ExBSFEkFINXH98NTrme5IOaaA7kIfiu2L8A3qhH9zRbukdCqdsA98TdElyeMe5BI8Rs2xHRIsoTSSVFfCFCWGPn9XHb4cdobRIWABNf0add9jakDjQJpJ1bTXOJXnnRXHRf+dNL1ZV1MBRCXhMbaHqGI1JkKIL7+i8uffuP6wVQAzO7+qVEbF6NbS0LJureYcWXUUhH66nLR5rYmva+2tjRFtojkM2aD76HEGAD3tPtKM309FJg5j/K682ywcWJ3PASCcycH/22u+Bh7Aa0ehM2Fu4z0SAE81HF9RkB21c5bEn4Dzw+/qNOyXr3DCTQDMBOdhi4nAgiFDGCinIa2owCEChUwD8qzd03PG+qdW/4fDzjUMcE1ZpIAAAAASUVORK5CYII=');
  }

  div.callout-caution.callout-style-default .callout-title {
    background-color: #ffe5d0
  }

  </style>
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">Text Pre-processing</h1>
  <p class="subtitle">LIS 4/5693: Information Retrieval and Text Mining</p>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Dr.&nbsp;Manika Lamba 
</div>
</div>
</div>

</section>
<section id="text-pre-processing" class="slide level2 smaller">
<h2>Text Pre-Processing</h2>
<ul>
<li>Process of cleaning and transforming raw text into usable form</li>
<li>Removes noise and prepares text for analysis</li>
<li>Text normalization: transformation into a standard (canonic) form or any useful form, e.g., from non-standard language to standard
<ul>
<li>upper/lower casing; notation of acronyms</li>
<li>standard form of dates, time, and numbers</li>
<li>stress marks, quotation marks, punctuation,</li>
<li>spelling correction; emoticons, emoji, hashtags, web links</li>
<li>tokenization</li>
<li>lemmatization and stemming</li>
<li>other forms of text preparation, e.g., extraction from PDFs, structured files like XML, web crawl, etc.</li>
</ul></li>
<li>Text preprocessing ensures quality and meaningful analysis</li>
</ul>
<aside class="notes">
<p>In this week’s module we will discuss the foundation of text mining and natural language processing that is text pre-processing. Text pre-processing helps us in transforms raw text into structured, machine-readable format so algorithms can analyze it effectively.</p>
<p>Raw text, as it exists in documents, websites, or social media, is often messy. It contains noise, inconsistencies, and formatting variations that can confuse downstream text analysis algorithms. The main goal of text preprocessing is to remove noise and prepare the text for analysis, ensuring that the data is consistent, structured, and meaningful.</p>
<p>Some of the common examples of text normalization are shown on this slide. We will examine each of these in more detail in the following slides. In addition to these examples, text preprocessing may include extracting text from different sources, such as PDF files, structured formats like XML, or web data obtained through crawling or scraping. These formats often require additional cleaning before analysis.</p>
<p>Text preprocessing is essential because it directly affects the quality of your analysis. Poor preprocessing can lead to inaccurate results, while careful preprocessing ensures that your models can identify meaningful patterns and relationships in the text.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="text-pre-processing-1" class="slide level2 smaller">
<h2>Text Pre-Processing</h2>
<ul>
<li>Basic pipeline
<ul>
<li><code>document → paragraphs → sentences → words</code></li>
<li><code>words and sentences → POS tagging</code></li>
<li><code>sentences → syntactical and grammatical analysis</code></li>
</ul></li>
</ul>

<img data-src="images/5.png" class="quarto-figure quarto-figure-center r-stretch"><aside class="notes">
<p>The basic pipeline of text preprocessing is generally broken down into smaller more structured units so that computers can analyze them more efficiently and easily.</p>
<p>We typically begin with a document, which could be anything such as a research article, a webpage, a transcript, or a social media post. This document is first divided into paragraphs, which helps organize the text into meaningful sections.</p>
<p>Next, each paragraph is divided into sentences. This step is called sentence segmentation, and it allows us to analyze the structure and meaning of the text at the sentence level.</p>
<p>Then, each sentence is further divided into words, through a process called tokenization. Words are the most basic units used in many text mining and NLP tasks. At this stage, the computer begins to work with individual tokens instead of large blocks of text.</p>
<p>Once we have words and sentences, we can perform Part-of-Speech, or POS tagging. This process assigns a grammatical label to each word, such as noun, verb, adjective, or adverb. This helps the system understand the grammatical role each word plays in a sentence. For example, the word “run” could be a noun or a verb depending on context, and POS tagging helps distinguish that.</p>
<p>After that, we can perform syntactical and grammatical analysis, also called parsing. This step examines the relationships between words in a sentence and helps determine the structure of the sentence. For example, it identifies subjects, verbs, and objects, and shows how words depend on one another.</p>
<p>This pipeline illustrates how we move from unstructured text to structured linguistic information. Each step adds more information and structure, which allows machines to better understand and analyze language.</p>
<p>This structured representation is essential for more advanced tasks such as named entity recognition, sentiment analysis, text classification, and machine learning.</p>
<p>Thus, text preprocessing is a step-by-step process that transforms raw documents into structured, meaningful units that can be analyzed computationally.</p>
<p>This slide also highlights some of the most popular Python libraries used for text preprocessing. We will explore spaCy in more depth in this week’s lab assignment.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="important-terms" class="slide level2">
<h2>Important Terms</h2>
<ul>
<li><p>Corpus: Collection of documents</p></li>
<li><p>Token: Individual word unit</p></li>
<li><p>Term: Unique vocabulary word</p></li>
<li><p>Chunk: Text unit such as paragraph</p></li>
<li><p>Dictionary: List of words associated with categories</p></li>
<li><p>Bag of Words: Frequency-based representation</p></li>
</ul>
<aside class="notes">
<p>Before we go further, it is important to review some key terminology. We have already encountered a few of these terms in previous modules, but lets revisit them.</p>
<p>First, let’s talk about the term <strong>corpus</strong>. A corpus refers to a collection of documents that we want to analyze. These documents could be research articles, tweets, emails, books, or transcripts. For example, if you were analyzing all the discussion posts in this class, those posts together would form your corpus.</p>
<p>Next is the term <strong>token</strong>. A token is an individual unit of text, most commonly a word. When we break a sentence into words during tokenization, each word becomes a token. For example, the sentence “Text mining is useful” contains four tokens: <em>text</em>, <em>mining</em>, <em>is</em>, and <em>useful</em>.</p>
<p>Closely related is the term <strong>term</strong>. A term refers to a unique word in the corpus vocabulary. While tokens include every occurrence of words, terms represent the distinct words. For example, if the word “data” appears 100 times, it is one term but 100 tokens.</p>
<p>The next term is <strong>chunk</strong>. A chunk refers to a larger unit of text, such as a paragraph, section, or sentence. Chunking helps organize text into meaningful segments for analysis.</p>
<p>Another important concept is the <strong>dictionary</strong>, also called a lexicon. This is a list of words associated with categories or meanings. For example, a sentiment dictionary might contain words labeled as positive or negative. Dictionaries are often used in tasks like sentiment analysis or topic classification.</p>
<p>Finally, we have the <strong>bag of words</strong>, which is one of the simplest and most widely used text representation methods. In this approach, we represent text by counting how often each word appears, without considering grammar or word order. This allows us to convert text into numerical form so it can be used in machine learning models.</p>
<p>Understanding these terms is essential because they form the foundation for all text preprocessing and text mining tasks that we will cover throughout this course.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="levels-of-text-representation" class="slide level2 smaller">
<h2>Levels of Text Representation</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p><code>Lexical Level</code></p>
<ul>
<li><p>Characters</p></li>
<li><p>Words</p></li>
<li><p>Phrases</p></li>
</ul>
<p><code>Syntactic Level</code></p>
<ul>
<li>Grammar structure</li>
<li>Examples
<ul>
<li>Language models</li>
<li>Vector-space models</li>
</ul></li>
</ul>
</div><div class="column" style="width:50%;">
<p><code>Semantic Level</code></p>
<ul>
<li>Meaning</li>
<li>Context and relationships</li>
<li>Examples
<ul>
<li>collaborative tagging (Web 2.0)</li>
<li>ontologies</li>
</ul></li>
</ul>
</div></div>
<aside class="notes">
<p>Now, let’s look at the three main levels of text representation: <strong>lexical, syntactic, and semantic levels</strong>. These levels represent increasing depth in how computers understand and analyze text.</p>
<p>Let’s begin with the lexical level, which is the most basic level of representation. At this level, text is treated as individual components such as characters, words, and phrases. For example, the word “mining” can be broken down into individual characters like m, i, n, i, n, g, or treated as a single word token. We can also analyze phrases, such as “text mining,” which consist of multiple words. Most basic text preprocessing steps, such as tokenization, stopword removal, stemming, and lemmatization, occur at this lexical level.</p>
<p>The next level is the syntactic level, which focuses on the grammatical structure of sentences. Instead of just looking at individual words, this level examines how words relate to each other within a sentence. For example, it helps identify subjects, verbs, and objects, and how they are organized. Examples of syntactic-level representations include language models and vector-space models, which capture patterns in word usage and structure. This level allows machines to better understand sentence structure rather than just isolated words.</p>
<p>Finally, we have the semantic level, which is the most advanced level and focuses on the meaning of the text, including context and relationships between concepts. At this level, the goal is to understand what the text actually means, not just how it is structured. Examples include ontologies, which represent relationships between concepts, and collaborative tagging, such as tags used in social media or web platforms. These approaches help capture deeper meaning, relationships, and context.</p>
<p>In other words, lexical level focuses on basic text units, the syntactic level focuses on structure and grammar, and the semantic level focuses on meaning and context. As we move from lexical to semantic levels, the representation becomes more complex and more powerful, allowing more advanced text analysis and understanding.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="common-text-pre-processing-tasks" class="slide level2 smaller">
<h2>Common Text Pre-processing Tasks</h2>
<ol type="1">
<li><code>Tokenization</code></li>
</ol>

<img data-src="images/clipboard-2616742373.png" class="quarto-figure quarto-figure-center r-stretch" width="500"><aside class="notes">
<p>Now, we will discuss some of the common text preprocessing tasks or process, starting with tokensization.</p>
<p>Tokenization is the process of breaking down raw text into smaller units called tokens. These tokens are usually individual words, but they can also be sentences, phrases, or even characters, depending on the task.</p>
<p>In the example shown on the slide, we begin with full sentences at the top. These sentences are natural language text, just as they would appear in a book or document. However, computers cannot directly analyze large blocks of text efficiently. So, the first step is to break the text into smaller, manageable pieces.</p>
<p>After tokenization, each sentence is split into individual words, such as “grew,” “pretty,” “little,” “tree,” and so on. Each of these words becomes a separate token. This allows the computer to process and analyze each word independently.</p>
<p>Tokenization is important because it serves as the foundation for almost all other text preprocessing steps. For example, before we can remove stopwords, count word frequencies, perform stemming or lemmatization, or apply machine learning models, we must first split the text into tokens.</p>
<p>It is also important to note that tokenization may involve removing punctuation, converting text to lowercase, and handling special cases such as contractions or abbreviations, depending on the tokenizer being used.</p>
<p>There are different types of tokenization. The most common are word tokenization, which splits text into words, and sentence tokenization, which splits text into sentences.</p>
<p>Thus, tokenization transforms raw, continuous text into structured units that can be analyzed computationally. It is the essential first step that enables all downstream text mining and natural language processing tasks.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="common-text-pre-processing-tasks-1" class="slide level2 smaller">
<h2>Common Text Pre-processing Tasks</h2>
<ol start="2" type="1">
<li><code>Lemmatization</code> and <code>Stemming</code></li>
</ol>
<p><img data-src="images/clipboard-3746816366.png" class="quarto-figure quarto-figure-center" width="500"> <img data-src="images/1.png" class="quarto-figure quarto-figure-center" width="500"></p>
<aside class="notes">
<p>Next, we will look at two important text preprocessing techniques: stemming and lemmatization. Both methods are used to reduce words to their base or root form, which helps standardize the text and improve analysis.</p>
<p>Let’s first understand why this is necessary. In natural language, words often appear in multiple forms. For example, words like “study,” “studies,” and “studying” all refer to the same core concept. If we treat them as separate words, it increases the size of our vocabulary unnecessarily and can reduce the effectiveness of our analysis. Stemming and lemmatization help solve this problem by reducing these variations to a common base form.</p>
<p>Let’s start with stemming. Stemming is a simpler and faster method that removes prefixes or suffixes to produce a root form, called the stem. However, the stem is not always a real word. For example, as shown on the slide, the words “change,” “changing,” and “changed” may all be reduced to “chang,” which is not a proper English word. Similarly, “studies” may be reduced to “studi.” Stemming focuses on mechanical rules rather than understanding meaning or grammar.</p>
<p>On the other hand, lemmatization is a more advanced approach. Lemmatization reduces words to their correct dictionary form, called the lemma. Unlike stemming, lemmatization considers the context and grammatical role of the word. For example, “was” becomes “be,” and “studies” and “studying” both become “study.” As shown on the slide, lemmatization produces meaningful, valid words.</p>
<p>The key difference is that stemming is faster but less accurate, while lemmatization is slower but more accurate and linguistically correct.</p>
<p>In practice, lemmatization is generally preferred when accuracy and interpretability are important, such as in research, sentiment analysis, or topic modeling. Stemming may be used when speed is critical, such as in large-scale search engines. Both stemming and lemmatization help reduce word variations, improve consistency, reduce vocabulary size, and enhance the performance of text mining and machine learning models.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="common-text-pre-processing-tasks-2" class="slide level2 smaller">
<h2>Common Text Pre-processing Tasks</h2>
<ol start="3" type="1">
<li><p><code>Stopwords</code></p>
<div class="columns">
<div class="column" style="width:50%;">
<p><img data-src="images/clipboard-2907913521.png" width="500"></p>
</div><div class="column" style="width:50%;">
<p><img data-src="images/clipboard-2478166995.png" width="500"></p>
</div></div></li>
</ol>
<aside class="notes">
<p>Another important text preprocessing step is called stopword removal. Stopwords are very common words that appear frequently in a language but typically carry little meaningful information for text analysis. Examples of stopwords include words such as “the,” “is,” “and,” “of,” “to,” and “in.” You can see a list of many common stopwords displayed on the slide.</p>
<p>These words are essential for human communication because they help form grammatically correct sentences. However, from a computational perspective, they often do not help distinguish between documents or provide meaningful insights into the content.</p>
<p>For example, in the sentence shown on the slide, “The quick brown fox jumps over the lazy dog,” words like “the” and “over” are stopwords. When we remove these stopwords, we are left with “quick brown fox jumps lazy dog.” This version retains the key meaningful words while removing less informative ones.</p>
<p>Removing stopwords has several benefits. First, it reduces the size of the vocabulary, which makes processing faster and more efficient. Second, it helps improve the performance of text analaysis tasks by focusing on the words that carry more meaningful information. Third, it reduces noise in the data and improves the quality of analysis.</p>
<p>However, it is important to note that stopword removal is not always appropriate. In some cases, stopwords may carry important meaning. For example, in sentiment analysis, words like “not” can completely change the meaning of a sentence. For example, “good” and “not good” have very different meanings. Removing the word “not” would lead to incorrect interpretation.</p>
<p>Most natural language processing libraries, such as spaCy and NLTK, provide predefined stopword lists, but these lists can also be customized depending on the specific application.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="common-text-pre-processing-tasks-3" class="slide level2 smaller">
<h2>Common Text Pre-processing Tasks</h2>
<ol start="4" type="1">
<li><p><code>Named Entity Recognization (NER)</code></p>
<p><img data-src="images/clipboard-2618144470.png"></p></li>
</ol>
<aside class="notes">
<p>Another common text preprocessing task is Named Entity Recognition or NER. NER is the process of automatically identifying and classifying important real-world entities in text. These entities typically include categories such as persons, organizations, locations, dates, times, and numerical values.</p>
<p>In the example shown on the slide, you can see a news article where different words and phrases are highlighted in different colors. Each color represents a different type of entity. For example, names like “Elon Musk” or “Ravi Kant Kumar” are identified as persons, organizations like “Reuters” are labeled as organizations, and references such as “June 14” or “345 pm” are labeled as dates and times.</p>
<p>This process helps convert unstructured text into structured information. Instead of just seeing a block of text, the computer can now recognize and categorize important elements within it.</p>
<p>NER is extremely useful in many real-world applications. For example, it is used in search engines to identify important keywords, in news analysis to extract people and organizations, in chatbots to understand user input, and in information extraction systems to build structured databases from text.</p>
<p>NER also improves other NLP tasks such as document classification, question answering, and knowledge graph construction, because it helps the system focus on meaningful entities rather than just individual words.</p>
<p>Most modern NLP libraries, such as spaCy, provide pre-trained models that can automatically identify these entities. In today’s lab, you will also see how spaCy can perform Named Entity Recognition on real text.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="common-text-pre-processing-tasks-4" class="slide level2 smaller">
<h2>Common Text Pre-processing Tasks</h2>
<ol start="5" type="1">
<li><code>Part-of-Speech (POS) Tagging</code></li>
</ol>
<p>POS tagging marks words in the corpus to a corresponding word based on its context and definition</p>

<img data-src="images/2.png" class="quarto-figure quarto-figure-center r-stretch" width="500"><aside class="notes">
<p>Now, lets discuss Part-of-Speech tagging, commonly called POS tagging, which is another important step in text preprocessing and linguistic analysis.</p>
<p>POS tagging is the process of assigning a grammatical label to each word in a sentence based on its context and role. These labels help identify whether a word is a noun, verb, adjective, adverb, or another part of speech.</p>
<p>For example, in the sentence shown on the slide, “Alice wrote a program,” each word has a specific grammatical role. “Alice” is labeled as a noun because it represents a person. “Wrote” is a verb because it describes an action. “A” is an article, and “program” is a noun representing an object.</p>
<p>POS tagging is important because many words can have different meanings depending on how they are used. For example, the word “book” can be a noun, as in “I read a book,” or a verb, as in “I will book a ticket.” POS tagging helps determine the correct meaning based on the sentence context.</p>
<p>You can also see examples of common POS tags used in the Python library NLTK shown on the slide. For example, “NNP” represents a proper noun, such as a person’s name. “NN” represents a singular noun. “VBD” represents a verb in past tense. “JJ” represents an adjective, and “RB” represents an adverb.</p>
<p>POS tagging plays a critical role in many advanced NLP tasks as it helps improve syntactic parsing, named entity recognition, sentiment analysis, and machine translation. It also helps computers understand sentence structure more accurately.</p>
<p>Most modern NLP libraries, including spaCy and NLTK, can automatically assign POS tags to text using pre-trained models.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="common-text-pre-processing-tasks-5" class="slide level2 smaller">
<h2>Common Text Pre-processing Tasks</h2>
<ol start="6" type="1">
<li><code>Bag of Words</code></li>
</ol>

<img data-src="images/clipboard-4172642049.png" class="quarto-figure quarto-figure-center r-stretch" width="500"><aside class="notes">
<p>Bag of Words model, often abbreviated as BoW, is one of the simplest and most widely used methods for representing text in a numerical form.</p>
<p>The key idea behind Bag of Words is to represent a document based on the frequency of words it contains, without considering grammar, sentence structure, or word order. In other words, the document is treated like a “bag” of individual words, where only the presence and count of words matter.</p>
<p>As shown in the example on the slide, we start with a sentence: “Futures markets opened higher today on news of overseas markets.” The first step is to preprocess the text, which typically includes removing stopwords such as “today,” “on,” and “of,” because they do not carry significant meaning for analysis.</p>
<p>After removing stopwords, we then group the remaining words and count how many times each word appears. This produces a frequency table. For example, the word “markets” appears twice, while words like “futures,” “opened,” “higher,” and “overseas” appear once.</p>
<p>This process converts the text into a structured, numerical format, which is necessary because machine learning algorithms cannot directly work with raw text. They require numerical input.</p>
<p>One important characteristic of the Bag of Words model is that it ignores word order and context. For example, the sentences “markets opened higher” and “higher opened markets” would produce the same Bag of Words representation. This makes the model simple and efficient, but it also means that it does not capture deeper meaning or relationships between words.</p>
<p>Despite this limitation, Bag of Words is very useful and is commonly used in tasks such as document classification, spam detection, sentiment analysis, and topic modeling.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="common-text-pre-processing-tasks-6" class="slide level2 smaller">
<h2>Common Text Pre-processing Tasks</h2>
<ol start="7" type="1">
<li><code>Term-Document Matrix</code></li>
</ol>
<ul>
<li>It represents terms as a table or matrix of numbers for a given corpus</li>
<li>In TDM, terms are represented as rows and documents as columns for a corpus where the number of occurrences of terms in the document is entered in the boxes</li>
</ul>

<img data-src="images/clipboard-2558688746.png" class="quarto-figure quarto-figure-center r-stretch" width="500"><aside class="notes">
<p>Term-Document Matrix or TDM is an important way to represent text in a structured, numerical format. A Term-Document Matrix is essentially a table that shows how frequently each term appears in each document within a corpus. This allows us to convert text into numbers, which is necessary for machine learning and text analysis.</p>
<p>As shown in the example on the slide, we begin with a small corpus consisting of three documents. Each document contains some text, such as “text analysis is fun,” or “I like doing text analysis.”</p>
<p>After preprocessing steps like tokenization and stopword removal, we identify the unique terms across all documents. These terms become the rows of the matrix, and the documents become the columns of the matrix.</p>
<p>Each cell in the matrix contains a number that represents the frequency of a specific term in a specific document. For example, if the word “text” appears once in document 1 and once in document 2, but not in document 3, the matrix will show values of 1, 1, and 0 in the corresponding row.</p>
<p>This matrix representation is extremely useful because it transforms unstructured text into a structured numerical form that can be used by machine learning algorithms.</p>
<p>The Term-Document Matrix is closely related to the Bag of Words model, since it is essentially the structured implementation of word frequency counts across multiple documents.</p>
<p>This representation is used in many applications, including document classification, topic modeling, clustering, and similarity analysis.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="common-text-pre-processing-tasks-7" class="slide level2 smaller">
<h2>Common Text Pre-processing Tasks</h2>
<ol start="8" type="1">
<li><code>Document-Term Matrix</code></li>
</ol>
<ul>
<li>It represents terms as a table or matrix of numbers for a given corpus</li>
<li>It is a transposition of TDM</li>
<li>In DTM, each document is a row, and each word is the column</li>
</ul>

<img data-src="images/clipboard-750409737.png" class="quarto-figure quarto-figure-center r-stretch" width="500"><aside class="notes">
<p>Next is Document-Term Matrix or DTM, which is another important way to represent text in numerical form for analysis.</p>
<p>A Document-Term Matrix is very similar to the Term-Document Matrix. The main difference is the orientation of the rows and columns. In a Document-Term Matrix, each document is represented as a row, and each term, or word, is represented as a column.</p>
<p>As shown in the example on the slide, we begin with a small corpus of three documents. After preprocessing steps such as tokenization and removing stopwords, we identify the unique terms across all documents. These unique terms form the columns of the matrix.</p>
<p>Each row represents one document, and each cell contains the frequency of a specific term in that document. For example, if the word “text” appears once in document 1 and once in document 2, but not in document 3, the matrix will show values of 1, 1, and 0 in the corresponding column.</p>
<p>The Document-Term Matrix is essentially the transpose of the Term-Document Matrix, meaning the rows and columns are swapped. Both representations contain the same information, but the Document-Term Matrix is often more convenient for machine learning applications because many algorithms expect data in the form of rows as observations and columns as features.</p>
<p>This representation allows us to convert unstructured text into structured numerical data that can be used for tasks such as classification, clustering, similarity analysis, and topic modeling.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="common-text-pre-processing-tasks-8" class="slide level2 smaller">
<h2>Common Text Pre-processing Tasks</h2>
<ol start="9" type="1">
<li><code>Term Frequency-Inverse Document Frequency (TF-IDF)</code></li>
</ol>
<ul>
<li>It evaluates the relevancy of a term for a document in a corpus and is the most popular weighting scheme in information retrieval (IR)</li>
<li>The term weighting is popularly used in IR and supervised machine learning tasks like text classification</li>
<li>It makes a list of more discriminative terms than others and assigns a weight to each highly occurring term</li>
</ul>

<img data-src="images/clipboard-3413337943.png" class="quarto-figure quarto-figure-center r-stretch" width="500"><aside class="notes">
<p>Term Frequency–Inverse Document Frequency, commonly known as TF-IDF, is one of the most important and widely used techniques in text preprocessing and information retrieval.</p>
<p>TF-IDF is a method used to measure how important or relevant a word is to a specific document within a corpus. Unlike simple word counts, TF-IDF does not treat all words equally. Instead, it assigns different weights to words based on their importance.</p>
<p>TF-IDF consists of two main components: Term Frequency, or TF, and Inverse Document Frequency, or IDF.</p>
<p>Term Frequency measures how often a word appears in a particular document. The idea is that words that appear more frequently in a document are likely to be more important for describing that document.</p>
<p>However, some words may appear frequently across many documents, such as common terms like “data,” “system,” or “information.” These words may not be very useful for distinguishing one document from another. This is where Inverse Document Frequency comes in.</p>
<p>Inverse Document Frequency reduces the weight of words that appear frequently across many documents and increases the weight of words that are rare across the corpus. This helps highlight words that are more unique and informative.</p>
<p>By combining these two measures, TF-IDF assigns higher weights to words that appear frequently in a specific document but not frequently across all documents. These words are called discriminative terms, because they help distinguish one document from another.</p>
<p>TF-IDF is extremely useful in many applications, including search engines, document ranking, text classification, and recommendation systems. For example, search engines use TF-IDF to determine which documents are most relevant to a user’s query.</p>
<p>Compared to simple Bag-of-Words counts, TF-IDF provides a more meaningful representation because it emphasizes important words and reduces the influence of common words.</p>
<p>TF-IDF is a powerful weighting technique that helps identify the most relevant and informative words in a document, improving the performance of text mining techniques.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="common-text-pre-processing-tasks-9" class="slide level2 smaller">
<h2>Common Text Pre-processing Tasks</h2>
<ol start="10" type="1">
<li><code>Word Embeddings</code></li>
</ol>
<div class="columns">
<div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="images/4.png" class="quarto-figure quarto-figure-center" width="500"></p>
</figure>
</div>
</div><div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="images/3.png" class="quarto-figure quarto-figure-center" width="500"></p>
</figure>
</div>
</div></div>
<aside class="notes">
<p>Finally, we have Word Embeddings, which represent one of the most advanced and powerful ways to represent text for computational analysis.</p>
<p>So far in this lecture, we have discussed methods like Bag of Words and TF-IDF, which represent text based on word counts and frequencies. While these methods are useful, they have an important limitation: they do not capture the meaning or context of words.</p>
<p>Word embeddings address this limitation by representing each word as a vector of numbers in a multi-dimensional space. These vectors capture the semantic meaning of words based on how they are used in context.</p>
<p>As shown on the top image on the slide, words with similar meanings appear closer together in this vector space. For example, the word “working” is located near related words such as “work,” “worker,” and “research.” This shows that the model has learned that these words are semantically related.</p>
<p>Unlike Bag of Words, which treats words independently, word embeddings capture relationships between words, allowing models to understand similarities, analogies, and context.</p>
<p>The bottom image shows two common methods used to generate word embeddings: Continuous Bag of Words, or CBOW, and Skip-gram. These are neural network models that learn word relationships from large text corpora.</p>
<p>CBOW predicts a target word based on its surrounding context words, while Skip-gram does the opposite — it predicts surrounding words based on a target word. Both methods help the model learn meaningful word representations.</p>
<p>Word embeddings are widely used in modern NLP applications such as search engines, recommendation systems, chatbots, machine translation, and large language models like the ones powering modern AI systems.</p>
<p>To conclude this presentation, text preprocessing plays a critical role in transforming raw text into structured, meaningful data. We began with basic steps such as tokenization, stopword removal, stemming, and lemmatization. We then explored methods for representing text numerically, including Bag of Words, Term-Document Matrix, TF-IDF, and finally word embeddings.</p>
<p>Each of these techniques builds upon the previous ones, allowing machines to better understand and analyze human language.</p>
<p>This week, you will apply many of these preprocessing techniques using the spaCy library, and gain hands-on experience transforming raw text into structured data for analysis.</p>
<p>I look forward to seeing how you apply these concepts in your own text mining projects!</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
<div class="quarto-auto-generated-content">
<p><img src="images/ou.png" class="slide-logo"></p>
<div class="footer footer-default">

</div>
</div>
</section>
    </div>
  </div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="index_files/libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="index_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="index_files/libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="index_files/libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="index_files/libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="index_files/libs/revealjs/plugin/reveal-chalkboard/plugin.js"></script>
  <script src="index_files/libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="index_files/libs/revealjs/plugin/notes/notes.js"></script>
  <script src="index_files/libs/revealjs/plugin/search/search.js"></script>
  <script src="index_files/libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="index_files/libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': false,
'previewLinksAuto': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleChalkboard(event)\"><kbd>b</kbd> Toggle Chalkboard</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleNotesCanvas(event)\"><kbd>c</kbd> Toggle Notes Canvas</a></li>\n<li class=\"slide-tool-item\" data-item=\"6\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.downloadDrawings(event)\"><kbd>d</kbd> Download Drawings</a></li>\n<li class=\"slide-tool-item\" data-item=\"7\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'chalkboard': {"buttons":true,"theme":"whiteboard"},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: true,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: "separate-page",

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, RevealChalkboard, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const isCodeAnnotation = (el) => {
        for (const clz of el.classList) {
          if (clz.startsWith('code-annotation-')) {                     
            return true;
          }
        }
        return false;
      }
      const onCopySuccess = function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copied!");
        let tooltip;
        if (window.bootstrap) {
          button.setAttribute("data-bs-toggle", "tooltip");
          button.setAttribute("data-bs-placement", "left");
          button.setAttribute("data-bs-title", "Copied!");
          tooltip = new bootstrap.Tooltip(button, 
            { trigger: "manual", 
              customClass: "code-copy-button-tooltip",
              offset: [0, -8]});
          tooltip.show();    
        }
        setTimeout(function() {
          if (tooltip) {
            tooltip.hide();
            button.removeAttribute("data-bs-title");
            button.removeAttribute("data-bs-toggle");
            button.removeAttribute("data-bs-placement");
          }
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      }
      const getTextToCopy = function(trigger) {
          const codeEl = trigger.previousElementSibling.cloneNode(true);
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
      }
      const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
        text: getTextToCopy
      });
      clipboard.on('success', onCopySuccess);
      if (window.document.getElementById('quarto-embedded-source-code-modal')) {
        // For code content inside modals, clipBoardJS needs to be initialized with a container option
        // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
        const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
          text: getTextToCopy,
          container: window.document.getElementById('quarto-embedded-source-code-modal')
        });
        clipboardModal.on('success', onCopySuccess);
      }
        var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
        var mailtoRegex = new RegExp(/^mailto:/);
          var filterRegex = new RegExp('/' + window.location.host + '/');
        var isInternal = (href) => {
            return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
        }
        // Inspect non-navigation links and adorn them if external
     	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
        for (var i=0; i<links.length; i++) {
          const link = links[i];
          if (!isInternal(link.href)) {
            // undo the damage that might have been done by quarto-nav.js in the case of
            // links that we want to consider external
            if (link.dataset.originalHref !== undefined) {
              link.href = link.dataset.originalHref;
            }
          }
        }
      function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
        const config = {
          allowHTML: true,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'light-border',
          placement: 'bottom-start',
        };
        if (contentFn) {
          config.content = contentFn;
        }
        if (onTriggerFn) {
          config.onTrigger = onTriggerFn;
        }
        if (onUntriggerFn) {
          config.onUntrigger = onUntriggerFn;
        }
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note) {
            return note.innerHTML;
          } else {
            return "";
          }
        });
      }
      const findCites = (el) => {
        const parentEl = el.parentElement;
        if (parentEl) {
          const cites = parentEl.dataset.cites;
          if (cites) {
            return {
              el,
              cites: cites.split(' ')
            };
          } else {
            return findCites(el.parentElement)
          }
        } else {
          return undefined;
        }
      };
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const citeInfo = findCites(ref);
        if (citeInfo) {
          tippyHover(citeInfo.el, function() {
            var popup = window.document.createElement('div');
            citeInfo.cites.forEach(function(cite) {
              var citeDiv = window.document.createElement('div');
              citeDiv.classList.add('hanging-indent');
              citeDiv.classList.add('csl-entry');
              var biblioDiv = window.document.getElementById('ref-' + cite);
              if (biblioDiv) {
                citeDiv.innerHTML = biblioDiv.innerHTML;
              }
              popup.appendChild(citeDiv);
            });
            return popup.innerHTML;
          });
        }
      }
    });
    </script>
    

</body></html>